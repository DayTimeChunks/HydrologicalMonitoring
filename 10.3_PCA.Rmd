---
title: "Principal Component Analysis (PCA)"
author: "PAZ"
date: "8 ao?t 2017"
output: pdf_document
---

## Libraries

```{r}
# Remove all R objects in the workspace
rm(list = ls())

require("vegan")
require("cluster")
require("gclus")

# install.packages("mvpart")
# require(mvpart)


library("ggplot2")
library("ggrepel")
library("MASS")
#library(plyr)
library("scales")
library("grid")

library("zoo")

# Melting data sets & changin axes
library("reshape2")
library("dplyr")


# Additional functions required for the following operations
source("coldiss.R")
source("panelutils.R")
# Function to compute a binary distance matrix from groups
source("grpdist.R")
# Function to draw ordered dendrograms with groups
source("hcoplot.R")
# Variance partitioning
source("vartest.R")
```


## Import data set to inspect (not standardized).

```{r}
# Check working directory
getwd()
# setwd("D:/Documents/these_pablo/Rscripts/Clustering")

sw = read.csv2("Data/WaterSoils_R.csv")

sw$Date.ti <- as.character(sw$Date.ti)
sw$Date.ti <- as.POSIXct(strptime(sw$Date.ti, "%Y-%m-%d %H:%M", tz="EST"))

```

## Standardization (transformation)

My intention is to transform data to aid comparability of variables (columns) with different magnitudes, scales and different quantities (hrs, m3/h, m3), I've settled for transformations employing both translation (substraction by a scalar quantity) and expansion (dividing (or multiplying) by a scalar quantity) available via the **scale()** function which does:

- **Z-scoring** (The mean of each variable is subtracted from the original values and the difference divided by the variable's standard deviation). Standardised values differ from one another in units of standard deviation, a key difference to ranging.


Variable choice:

- High cummulative rain during sampling, but relative low rainfall intensity controlled the event of May 29, likely leading to slow drainage transport of freshly applied product during the mid season.
- 

```{r}

names(sw)

swNew = 
  sw %>% dplyr::rename(P.Cum = CumRain.mm,
                P.Int = RainInt.mmhr,
                Q.Ave = AveDischarge.m3.h,
                Q.Ini = iflux,
                Q.Max = maxQ,
                Vol = Volume.m3,
                SSM = ExpMES.Kg,
                T.Hrs = Duration.Hrs)

names(swNew)

waterX <- swNew[c("Date.ti", "Events",
           # Dry Hours capture equivalent components to Duration
           #"dryHrsIni", 
           #"dryHrsMax", #Separates groups better than Average
           #"dryHrsAve", 
           #"noEventHrsIni", 
           #"noEventHrsMax", 
           #"noEventHrsAve", 
           "P.Cum", 
           "P.Int",  
           "Q.Max", # "minQ",
           "Q.Ave", # "chExtreme",
           "T.Hrs",
           "Vol", 
           # "DIa",
           "Q.Ini",  # Flux at the start of sampling
           #"fflux",
           # "chExtreme",  
           "SSM" # ,
           # "DD13C.diss",  "DD.diss.nrm", Needs to use NA.omit
           #"SM.g.nrm", "TP.g.nrm"
           )]

y <- sw[c("Date.ti", "Events",
          # Response variables
          # "Conc.mug.L", "OXA_mean", "ESA_mean", 
          "DD13C.diss" , 
          "DD.diss.nrm", "SM.g.nrm", "TP.g.nrm")]

names(waterX)

if ( class(waterX[, 2])== "factor") {
  # Hellinger
  waterX.hell <- decostand(waterX[, 3:ncol(waterX)], "hellinger", na.rm=T, MARGIN = 2) # Margin 2 = columns
  
  # Normalize to 1
  # make margin sum of squares equal to one (default MARGIN = 1)

    
  # Z-scoring [Mean = 0, SD = 1 (for every column)]
  waterX.z <- scale(waterX[, 3:ncol(waterX)])
}

# Chose standardization to test
std.Train <- 
   waterX.hell 
  # waterX.z
  # waterX[, 3:ncol(waterX)] # Not-scaled

# Test:
colMeans(waterX.z) # mean = 0
apply(waterX.z, 2, sd) # SD = 1

```

# Principal Component derivation methods

```{r}

# PCA Methods

# Metod prcomp
std.Train.pca <- prcomp(std.Train, retx=T, scale.=F) # Already scaled 
scores <- std.Train.pca$x[ ,1:2]

# Method - RDA
# PCA on a covariance matrix (default scale=FALSE)
std.Train.pca2 = rda(std.Train)

# Method - Princomp
# std.Train.pca3 <- princomp(std.Trai, cor=TRUE)
   
```


## Cluster ID's for biplot (not on PCA object)

```{r}

std.Train.daisy =  daisy(std.Train, "euclidean")
std.Train.clust = hclust(std.Train.daisy,  "ward.D")

# Cutting to 4 (not 2) to avoid SD associated to the 8-1 outlier event
k_hc <- cutree(std.Train.clust, 3)
k_hc

# Optimal clustering based on:
# Spearman's rank correlations
	# Ward/Hellinger
  # windows(16,8)
	# par(mfrow=c(1,2))
	Si = numeric(nrow(std.Train))
	for (k in 2:(nrow(std.Train)-1)) {                              
	sil = silhouette(cutree(std.Train.clust, k=k), std.Train.daisy)
	Si[k] = summary(sil)$avg.width
	}                       
	k.best = which.max(Si)
	plot(1:nrow(std.Train), Si, type="h", main="Silhouette-optimal number of clusters - Hellinger/Ward",
	xlab="k (number of groups)", ylab="Average silhouette width")
	axis(1, k.best, paste("optimum",k.best,sep="\n"), col="red", col.axis="red")


# clustering based on:
# k-means clustering [assume X? clusters]
km <- kmeans(scores, centers=3, nstart=10)
km$cluster

sum(km$cluster == 3)/length(km$cluster)

```


## Plotting with "cleanplotPCA"


### Scaling 1 

- Distances between object points approximate the Euclidean distances between objects. Thus, objects ordinated closer together can be expected to have similar variable values.
- The length of a variable vector in the ordination plot reflects its contribution to the ordination
- Angles between variable vectors are meaningless

### Scaling 2 

- The angles between all vectors approximate their (linear) covariance/correlation.
- Distances between object points may be non-Euclidean and should not be interpreted with great confidence.

```{r}


# ,echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3,fig.show='hold',fig.align='center'
####################################################################
source("cleanplotPCA.R")
# A function to draw two biplots (scaling 1 and scaling 2) from an object 
# of class "rda" (PCA or RDA result from vegan's rda() function)
#
# License: GPL-2 
# Authors: Francois Gillet & Daniel Borcard, 24 August 2012
#
# http://www.davidzeleny.net/anadat-r/doku.php/en:numecolr:cleanplot.pca
####################################################################

# Needs RDA (PCA object), hence ...pca2
cleanplot.pca(std.Train.pca2, point = T, 
              labs = waterX$Events, k = 3, dfcut = std.Train.clust, # Used hierarchical clust (hclust)
              cluster = TRUE)

```

## Plotting with GGPLOT

```{r}

# Scores of PCA object, 
scores <- std.Train.pca$x[ ,1:2]

# Data for GGplot
ggdata <- data.frame(scores, Cluster=km$cluster, Species=waterX$Events) # With k-means
# ggdata <- data.frame(scores, Cluster=k_hc , Species=waterX$Events) # With h-clust

ggdata$CompareLutz <- ifelse(ggdata$Species == "8-1", as.character(ggdata$Species), 
                               ifelse(ggdata$Species == "8-2", as.character(ggdata$Species), 
                                      ifelse(ggdata$Species == "8-3", as.character(ggdata$Species), 
                                              ifelse(ggdata$Species == "10-1", as.character(ggdata$Species), 
                                                      ifelse(ggdata$Species == "10-2", as.character(ggdata$Species), 
                                                              ifelse(ggdata$Species == "10-3", as.character(ggdata$Species), 
                                                                      ifelse(ggdata$Species == "12-1", as.character(ggdata$Species), 
                                                                             ifelse(ggdata$Species == "12-2", as.character(ggdata$Species),
                                                                                    ifelse(ggdata$Species == "12-3", as.character(ggdata$Species),
                                                                                           NA)))))))))




ggdata$QuantPCA <- ifelse( (scores[,1] > 0 | scores[,2] > 1) 
                           # & ggdata$Cluster != 3
                           & ggdata$Cluster != 1
                           & !is.na(y$DD13C.diss), as.character(ggdata$Species), NA)


ggdata$Quantif <- ifelse(!is.na(y$DD13C.diss), as.character(ggdata$Species), NA)
# stat_ellipse is not part of the base ggplot package
# source("https://raw.github.com/low-decarie/FAAV/master/r/stat-ellipse.R") 

# Relabeling based on new event classification
split <- strsplit(ggdata$QuantPCA, "-", fixed = TRUE)
ggdata$evTemp <- sapply(split, "[", 1)
ggdata$evTemp2 <- sapply(split, "[", 2)
ggdata$Label = ifelse(ggdata$evTemp == 8, "A", 
                      ifelse(ggdata$evTemp == 10, "B",
                             ifelse(ggdata$evTemp == 11, "C",
                                    ifelse(ggdata$Species == "12-1", "D",
                                           ifelse(ggdata$Species == "12-2" | ggdata$Species == "12-3", "E",
                                                  ifelse(ggdata$evTemp == 15, "F",
                                                         ifelse(ggdata$evTemp == 18, "G", NA)
                                                         )
                                                  )
                                           )
                                    )
                             )
                      )
ggdata$evTemp2 <- ifelse(ggdata$Species == "12-2" | ggdata$Species == "15-2", "1", 
                         ifelse(ggdata$Species == "12-3", "2", ggdata$evTemp2))
ggdata$EventLabel = ifelse(!is.na(ggdata$Label) , paste(ggdata$Label, ggdata$evTemp2, sep = "-"), NA)
ggdata$evTemp = NULL
ggdata$evTemp2 = NULL

library(proto)
library(ggplot2)
ggplot(ggdata) +
  geom_point(aes(x=PC1, y=PC2, color=factor(Cluster)) , size=5, shape=20) +
   theme_bw() +
  stat_ellipse(aes(x=PC1,y=PC2,fill=factor(Cluster)),
               geom="polygon", level=0.69, alpha=0.2) +
  guides(color=guide_legend("Cluster"), fill=guide_legend("Cluster")) +
 
  # geom_text_repel(aes(x=PC1, y=PC2, label=CompareLutz),
  geom_text_repel(aes(x=PC1, y=PC2, 
                      # label=CompareLutz), 
                      label=EventLabel),
                 arrow = arrow(length = unit(0.005, 'npc'), type = "closed"),
                 force = 1, 
                 point.padding = unit(1.0, 'lines'), 
                 max.iter = 2e3,
                 nudge_x = .2)



  
```


# Extracting the ggbiplot code to ggplot:

```{r}

# Number of observations factor
nobs.factor <- sqrt(nrow(std.Train.pca$x) - 1)
d <- std.Train.pca$sdev # SD
u <- sweep(std.Train.pca$x, MARGIN = 2, STATS = 1 / (d * nobs.factor), FUN = '*') 
    # Sweep = Return an array obtained from an input array by sweeping out a summary statistic
    # sweep(x, MARGIN, STATS, FUN = "-", check.margin = TRUE, ...)
v <- std.Train.pca$rotation # Rotation

# Scores
  choices <- pmin(choices = 1:2, ncol(u)) # choices = 1:2
# Scaling to use (0 or 1)
# When scale = 1, the inner product between the variables approximates the covariance and the distance between the points approximates the Mahalanobis distance.
  scale = 1 
  obs.scale = 1 - scale
  df.u <- as.data.frame(sweep(u[,choices], 2, d[choices]^obs.scale, FUN='*')) # Only two components

# Directions
  var.scale = scale # scale factor to apply to variables
  v <- sweep(v, 2, d^var.scale, FUN='*')
  df.v <- as.data.frame(v[, choices])
  
  # Axes names (changed from PC1 and PC2)
  names(df.u) <- c('xvar', 'yvar')
  names(df.v) <- names(df.u)

# Scale the radius of the correlation circle so that it corresponds to 
  # a data ellipse for the standardized PC scores
  circle.prob = 0.60
  r <- sqrt(qchisq(circle.prob, df = 2)) * prod(colMeans(df.u^2))^(1/4)
  
# Scale directions (variable arrows)
  v.scale <- rowSums(v^2)
  df.v <- r * df.v / sqrt(max(v.scale))
  
# Change the labels for the axes
  if(obs.scale == 0) { # Scaling 1
    u.axis.labs <- paste('Standardized PC', choices, sep='')
  } else { # Scaling 2
    u.axis.labs <- paste('PC', choices, sep='')
  }

# Append the proportion of explained variance to the axis labels
  u.axis.labs <- paste(u.axis.labs, 
                       sprintf('(%0.1f%% explained var.)', 
                               100 * std.Train.pca$sdev[choices]^2/sum(std.Train.pca$sdev^2)))
  
# Score Labels
  labels = NULL
  labels.size = 3
  alpha = 1
if(!is.null(labels)) {
    df.u$labels <- labels
    }
  
  # Grouping variable
  # groups = as.factor(km$cluster)
  # if(!is.null(groups)) {
  #   df.u$groups <- groups
  # }
  
  # Variable Names (Abbreviate or not)
  varname.abbrev = FALSE
  if(varname.abbrev) {
    df.v$varname <- abbreviate(rownames(v))
  } else {
    df.v$varname <- rownames(v)
  }
  
  # Variables for text label placement
  varname.adjust = 2
  df.v$angle <- with(df.v, (180/pi) * atan(yvar / xvar))
  df.v$hjust = with(df.v, (1 - varname.adjust * sign(xvar)) / 2)
  
##################  
# Base plot
  
  
  df.u$Cluster = as.factor(km$cluster)
  
  g <- ggplot(data = df.u, aes(x= xvar, y=yvar)) + # , aes(x = xvar, y = yvar)
    geom_point(aes(group = Cluster , col = Cluster), size=5, shape=20) + # x= PC1, y=PC2 
    xlab(u.axis.labs[1]) + ylab(u.axis.labs[2]) # + coord_equal()
  
  var.axes = TRUE
  varname.size = 3.5
  circle = TRUE
  if(var.axes) {
    # Draw circle
    if(circle) 
    {
      theta <- c(seq(-pi, pi, length = 50), seq(pi, -pi, length = 50))
      circle <- data.frame(xvar = r * cos(theta), yvar = r * sin(theta))
      g <- g + geom_path(data = circle, color = muted('white'), 
                         size = 1/2, alpha = 1/3)
    }
    
    # Draw directions & names
    g <- g +
      geom_segment(data = df.v,
                   aes(x = 0, y = 0, xend = xvar, yend = yvar),
                   arrow = arrow(length = unit(1/2, 'picas')), 
                   color = muted('red')) +
      geom_text(data = df.v, 
                aes(label = varname, x = xvar, y = yvar, 
                    angle = angle, hjust = hjust), 
                color = 'darkred', size = varname.size #,
                # position=position_jitter(width=.02,height=0.02)
                )
  }
  
  
  # Overlay a concentration ellipse if there are groups
  ellipse = FALSE
  ellipse.prob = 0.95
  if(!is.null(df.u$Cluster) && ellipse) {
    theta <- c(seq(-pi, pi, length = 50), seq(pi, -pi, length = 50))
    circle <- cbind(cos(theta), sin(theta))
    
    ell <- ddply(df.u, 'Cluster', function(x) {
      if(nrow(x) <= 2) {
        return(NULL)
      }
      sigma <- var(cbind(x$xvar, x$yvar))
      mu <- c(mean(x$xvar), mean(x$yvar))
      ed <- sqrt(qchisq(ellipse.prob, df = 2))
      data.frame(sweep(circle %*% chol(sigma) * ed, 2, mu, FUN = '+'), 
                 Cluster = x$Cluster[1])
    })
    names(ell)[1:2] <- c('xvar', 'yvar')
    g <- g + geom_path(data = ell, aes(color = Cluster, group = Cluster)) 
      
  }
  
  # Label the variable axes
  #if(var.axes) {
  #  g <- g + 
  #    geom_text(data = df.v, 
  #              aes(label = varname, x = xvar, y = yvar, 
  #                  angle = angle, hjust = hjust), 
  #              color = 'darkred', size = varname.size)
  #}
  fg <-
    g + 
    stat_ellipse(data= subset(df.u, xvar < 0.4 ), aes(x=xvar,y=yvar, fill= Cluster),
               geom="polygon", level=0.90, alpha=0.2) +
    theme_minimal() +
  # geom_point(aes(x=std.Train.pca$x[,1], y=std.Train.pca$x[,2], col = as.factor(km$cluster), size = 0.2)) +
  geom_text_repel(aes(x= xvar, y=yvar, 
                      label=ggdata$EventLabel),
#                      label=ggdata$CompareLutz),
                 arrow = arrow(length = unit(0.01, 'npc'), type = "closed"),
                 force = 1, 
                 point.padding = unit(1.5, 'lines'), 
                 max.iter = 2e3,
                 nudge_x = 0.01 #,
                 # position=position_jitter(width=.02,height=0.02)
                 )
  fg
# ggsave(fg, filename = "Images/PCA_events_C1early.png", width = 8, height = 5, units = "in", scale = 1)
# ggsave(fg, filename = "Images/PCA_events_C1late.png", width = 8, height = 5, units = "in", scale = 1)
```


## Plotting with ggbiplot (arrows with ggplot)

```{r}

# install.packages("devtools")
library("devtools")

# install_github("vqv/ggbiplot")
library("ggbiplot")

biplot(std.Train.pca)

## Tweak GGbiplot for desired result.
# std.Train.pca2 <- prcomp(std.Train, retx=T, scale.=T) 
ggbiplot(std.Train.pca, obs.scale = 1, var.scale = 1, # Scale 0 = scaling 1 in lit; Scale 1 = scaling 2 in lit.
  groups = as.factor(km$cluster), # km$cluster
  varname.abbrev = T,
  varname.adjust = 1.5,
  ellipse = TRUE, 
  circle = FALSE, 
  ellipse.prob = 0.95) +
  # scale_color_discrete(name = '') +
  theme(legend.direction = 'horizontal', legend.position = 'top') +
  theme_bw() +
  # geom_point(aes(x=std.Train.pca$x[,1], y=std.Train.pca$x[,2], col = as.factor(km$cluster), size = 0.2)) +
  geom_text_repel(aes(x=std.Train.pca$x[,1], y=std.Train.pca$x[,2], label=ggdata$CompareLutz),
                 arrow = arrow(length = unit(0.005, 'npc'), type = "closed"),
                 force = 1, 
                 point.padding = unit(1.0, 'lines'), 
                 max.iter = 2e3,
                 nudge_x = 2)


# std.Train.pca$x[,1]
```


## Environmental interpretation

```{r}

# A posteriori interpretation of the species by significative environmental variables
## Selection of the significant variables      
# windows(8,8)                                    
# par(mfrow=c(1,1)) 

#fit = envfit(std.Train.pca2, std.Train.nona, perm=1000)                                       
#fit

#plot(std.Train.pca2, type="t", main=paste("PCA/Hellinger"))       
#plot(fit, axis=T) 

# std.Train.pca2
```

## Summary of Eigenvalues

Plot the eigen values for each component and a line depicting the mean eigen value, below which components will not be considered.

```{r}

# Method 2
# Automatic scaling with scale = "TRUE" (no need for earlier transformations/normalization)

summary(std.Train.pca2, scaling = 1)

# Eigen values
(ev <- std.Train.pca2$CA$eig)

# Percentage of variance for each axis
100*ev/sum(ev)

# Apply Kaiser's rule to select axes
ev[ev > mean(ev)] 

# Plot eigen values and % variance for each axis
barplot(ev, main = "Eigenvalues for PCA on ENV", col = "bisque", las=2)
abline(h=mean(ev), col = "blue")
legend("topright", "Average Eigenvalue", lwd = 1, col = "blue", bty = "n")
```


## Merge ggdata with dissolved isotope values

```{r}


yClust <- merge(waterX, y, by = "Events")
#  g = ggdata %>% rename(Events = Species) # detach plyr first
names(ggdata)[names(ggdata) == "Species"] <- "Events"
yClust <- merge(yClust, ggdata, by = "Events")

write.csv2(yClust, 
           'Data/PCA4Lutz_R.csv', row.names = F)


```



