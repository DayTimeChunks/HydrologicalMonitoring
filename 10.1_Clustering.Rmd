---
title: "Clustering Techniques"
author: "PAZ"
date: "10 avril 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)
Sys.setlocale("LC_ALL", "English")
```

## Files

Imports: 

- **WaterSoils_R.csv**

## Import packages

```{r }
# Preparation of the workspace
# Remove all R objects in the workspace
rm(list = ls())

# ipak function: install and load multiple R packages.
# check to see if packages are installed. Install them if they are not, then load them into the R session.
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

# usage
# packages <- c("vegan", "cluster", "gclus", "MASS")
# ipak(packages)

# Load required libraries
require("vegan")
require("cluster")
require("gclus")

library("ggplot2")
library("ggrepel")
library("MASS")

library("zoo")

# Melting data sets & changin axes
library("reshape2")

```

## Import data set to inspect

```{r}
# Check working directory
getwd()
# setwd("D:/Documents/these_pablo/Rscripts/Clustering")

sw = read.csv2("Data/WaterSoils_R.csv")

sw$Date.ti <- as.character(sw$Date.ti)
sw$Date.ti <- as.POSIXct(strptime(sw$Date.ti, "%Y-%m-%d %H:%M", tz="EST"))

```

## Standardization (transformation)

Occasionally, the variables in a "raw" data set have properties that violate an assumption of a statistical procedure (e.g. normally distributed values) or which cannot be compared to other variables due to differences in scale or variability. For example, principal components analysis (PCA) requires that variables be linearly related to one another and on roughly the same scale or will perform poorly. Rather than abandoning an analysis due to inappropriate data structure, it may be possible to transform the variables so they satisfy the conditions in question. A transformation involves the application of a mathematical procedure to every value of a given variable or set of variables to create a new set of values. The new values of the transformed variables should still represent the data, but will be more amenable to analysis or comparison.

## Problem and doubts

Originally I used "ecologically motivated" transformations available in the **decostand()** package. Eg:

- total: divide by margin total (default MARGIN = 1)
- hellinger: square root of method = "total" (Legendre & Gallagher 2001).
- normalize: make margin sum of squares equal to one (default MARGIN = 1).
- chi.square: divide by row sums and square root of column sums, and adjust for square root of matrix total (Legendre & Gallagher 2001).

However, I am unsure about whether the transformations I want to make are relevant to those that are "ecologically motivated". So, given that my original motivation was to transform data to aid comparability of variables (columns) with different magnitudes, scales and different quantities (hrs, m3/h, m3), I've settled for transformations employing both translation (substraction by a scalar quantity) and expansion (ividing (or multiplying) by a scalar quantity) available via the **scale()** function which does:

- **Z-scoring** (The mean of each variable is subtracted from the original values and the difference divided by the variable's standard deviation). Standardised values differ from one another in units of standard deviation, a key difference to ranging.


```{r}

names(sw)
waterX <- sw[, c("Date.ti", "Events",
                 "dryHrsIni", "dryHrsMax", "dryHrsAve", 
                 "noEventHrsIni", "noEventHrsMax", "noEventHrsAve", 
                 "CumRain.mm", "RainInt.mmhr", 
                 "maxQ", "AveDischarge.m3.h",
                 "Duration.Hrs",
                 "Volume.m3" #, "DIa",
                 #"ExpMES.Kg" #, "Sequence"
                 #"DD13C.diss",  "DD.diss.norm", "SM.g.nrm.prc", "MEL.g.nrm.prc""
                 )]




y <- sw[c("Events",
          # Response variables
          # "Conc.mug.L", "OXA_mean", "ESA_mean", 
          "DD.diss.nrm", "SM.g.nrm", "TP.g.nrm")]

names(waterX)

if ( class(waterX[, 2])== "factor") {
  # Hellinger
  waterX.hell <- decostand(waterX[, 3:ncol(waterX)], "hellinger", na.rm=T, MARGIN = 2) # Margin 2 = columns
  
  # Normalize to 1
  # make margin sum of squares equal to one (default MARGIN = 1)

    
  # Z-scoring [Mean = 0, SD = 1 (for every column)]
  waterX.z <- scale(waterX[, 3:ncol(waterX)])
}

# Test:
colMeans(waterX.z) # mean = 0
apply(waterX.z, 2, sd) # SD = 1


```

# Dissimilarity matrix

See: https://sites.google.com/site/mb3gustame/reference/dissimilarity

*Note: The (dis)similarity matrix is computed directly in the NDMS function below (**metaMDS**), so not necessary here. However, it is done before as well to find otpimal clusters for plotting afterwards.* 

Steps following the Gustame dissimilarity wizard (https://sites.google.com/site/mb3gustame/wizards/-dis-similarity-wizard):

1. I'd like to know how (dis)similar my objects (rows) are
2. The variables (columns) describing my objects (rows) are more like: physicochemical data
3. Association is to be measured between: individual objects (not group of objects)
4. For my variables: partial (dis)similarities can be calculated between them (as opposed from being mutually exclusive or prescence/abscence)
5. Variables are: Quantitative and dimensionally homogeneous (i.e. have the same units) -> Yes, as they have been standardized above.

Results suggest an association for objects described by quantitative and homogeneous variables with four valid alternatives (chosen in **bold**):

I. **Euclidean distance (D1)**
II. Average distance (D2)
III. Manhattan metric (D7)
IV. Mean character difference (D8)

Note: In **D1** double zeros result in decreased distances. This property makes the Euclidean distance unsuitable for many ecological data sets and ecologically-motivated transformations should be considered. 

Package: **daisy()** (can compute a Gower coefficient for both quantitative and categorical variables)
Package: **dist()** (needed for the ordiplot)

```{r}
# Compute dissimilarity and distance matrices (Q mode)
waterX.z.daisy =  daisy(waterX.z, "euclidean")

```


## Clustering

Will be using "eucledian" distances, which are the root sum-of-squares of differences and the defining clusters via hierarchical clustering (hclust) using the "Ward" agglomeration method. Ward's method (Ward, 1963) determines which clusters to merge by evaluating the 'cost' of such a merge against an objective function. Merges with the minimum cost are performed at each stage of the algorithm. Typically, this is implemented by evaluating the sum of squared deviations from cluster centroids. Every possible merge is evaluated at each stage of the algorithm and that which yields the smallest increase in the sum of squared deviations is selected. 

### Hierarchical clustering and Ward agglomeration (minimum variance)

Find optimal clustering in the X-variables so as to identify groups to inspect. Not necessary for NDMS, but handy to graph afterwards.

```{r}

findOptimal <- function(x, x.hw, x.dh) {
  # 1st arg: dataframe
  # 2nd arg: clustered object
  # 3rd arg: transformed (e.g. normalized) data frame
  Si = numeric(nrow(x))
  
  for (k in 2:(nrow(x)-1)) {
    sil = silhouette(cutree(x.hw, k=k), x.dh)
    Si[k] = summary(sil)$avg.width
  }
  
  k.best = which.max(Si)
  plot(1:nrow(x), Si, type="h", main="Silhouette-optimal number of clusters - Hellinger/Ward",
       xlab="k (number of groups)", ylab="Average silhouette width")
  axis(1, k.best, paste("optimum",k.best,sep="\n"), col="red", col.axis="red")
}
# par(mfrow=c(2,1)) # Plot two rows, one column 

# Clusterings based on the species/rows distances 
waterX.z.clust = hclust(waterX.z.daisy,  "ward.D")

k <- findOptimal(waterX, waterX.z.clust, waterX.z.daisy)[1] # Optimal number of groups
gr = cutree(waterX.z.clust, k = k)

waterX <- cbind(gr, waterX)
names(waterX)[names(waterX) == "gr"] <- "Cluster"
```

## Dendogram plots

```{r}

# Plot dendrograms of Hellinger distance based clusterings
#windows(5,10)
# par(mfrow=c(1,1))

# The mar command defines plot margins in order bottom, left, up, right using
# row height (text height) as a unit.
par(mar=c(3,4,1,1)+.1)

#plot(resWater.hw, method ="ward.D", xlab="", sub="")
nrow(waterX) 
nrow(waterX.z.clust)

plot(waterX.z.clust, labels = waterX$Events, cex=0.6, 
     method ="ward.D", xlab="Hellinger", sub="")
rect.hclust(waterX.z.clust, k)


```

## Extracting labels by group and merging with original data

```{r}

# Parameteric
### T.test
#t.test(x,y, alternative = c("two.sided"),  conf.level = 0.975)
#t.test(group1$SM.g.nrm , group2$SM.g.nrm, alternative = c("two.sided"),  conf.level = 0.975)
#t.test(group1$maxQ , group2$maxQ, alternative = c("two.sided"),  conf.level = 0.975)

### MANOVA test
# Testing differences in cluster groups
### Raw response variables
colnames(bGrp.K.XY.hell)
colnames(bGrp.K.XY.norm)


res.man <- manova(cbind(
  # DD13C.diss, 
  DD.diss.nrm, # Percent is exactly the same: DD.diff.prc,
  SM.g.nrm,         
  TP.g.nrm, 
  # TotSMout.g, MELsm.g,  Conc.mug.L, OXA_mean, ESA_mean, 
  
  # Responses
  maxQ, Duration.Hrs, AveDischarge.m3.h, Volume.m3 #, 
  # DIa
  ) 
                  ~ Cluster, data = bGrp.K.XY.hell)
                  #~ Cluster, data = bGrp.K.XY.norm)

# Test for normality
# shapiro.test(bGrp.K.XY.hell$Conc.mug.L)
# shapiro.test(bGrp.K.XY.hell$TotSMout.g.x)
# shapiro.test(bGrp.K.XY.hell$OXA_mean)
shapiro.test(bGrp.K.XY.hell$DD.diss.nrm)
# shapiro.test(bGrp.K.XY.hell$DD13C.diss.x) # Not significant

# Number of obs per cluster
n1 <- lengths(subset(bGrp.K.XY.hell, !is.na(DD.diss.nrm) & Cluster == 1))[[1]]
n2 <- lengths(subset(bGrp.K.XY.hell, !is.na(DD.diss.nrm) & Cluster == 2))[[1]]
#n3 <- lengths(subset(bGrp.K.XY.hell, !is.na(DD.diss.norm) & Cluster == 3))[[1]]
#n4 <- lengths(subset(bGrp.K.XY.hell, !is.na(DD.diss.norm) & Cluster == 4))[[1]]


# boxplot(bGrp.K.XY.hell$DD.diss.norm)

### Look to see which differ
summary.aov(res.man)

plist <- c()
for (i in 1:length(summary.aov(res.man))) {
  name <- paste("Response:", i, sep = " ")
  tmp <- summary.aov(res.man)[[i]][["Pr(>F)"]][1]
  plist[[name]] <- tmp 
}

dropBox <- c("Events")

# Melt & but remove "Events" variable first
meltGroupK2 <- melt((bGrp.K.XY.hell[ , !(names(bGrp.K.XY.hell) %in% dropBox)]), id=c("Cluster"))

subGroupK2 <- subset(meltGroupK2, 
                     (
                       variable == 'DD13C.diss.x' 
                      | variable == 'DD.diss.norm'
                      | variable == 'Conc.mug.L'
                      | variable == 'TotSMout.g.x'
                      # | variable == 'MELsm.g.x'
                      | variable == 'OXA_mean'
                      | variable == 'ESA_mean'
                      | variable == 'maxQ'
                      | variable == 'AveDischarge.m3.h'
                      | variable == 'Volume.m3'
                      | variable == 'Duration.Hrs' 
                      | variable == "ExpMES.Kg" 
                      )
                     )
# Get p-values for graph
DD.diss_p <- plist[[1]]

groupBoxPlot <- ggplot(data = subGroupK2, mapping = aes(x = Cluster, y = value, fill = Cluster)) +
  facet_wrap(~variable, scales = "free") +
  geom_boxplot() +
  theme_bw()

groupBoxPlot
# ggsave(groupBoxPlot, filename = "groupBoxPlot.png", width = 8, height = 5, units = "in", scale = 1)

```

## Testing significance between response variables

Student t-test (more than 25 samples, parameteric) and Wilcoxon test (unpaired and 2-sided) (less than 25 samples, non-parameteric); test tends to be significant if the p-value is < 0.1 or better 0.05.

$H_o$: Difference between means is zero
$H_1$: Difference between means is not zero 

```{r}

### Normalized responses
#res.man.nrm <- manova(cbind(DD.diss.norm, 
#                            SM.g.nrm, MEL.g.nrm, 
#                            SM.g.nrm.prc, MEL.g.nrm.prc) ~ x, data = bGrp.K.XY.hell)
#summary.aov(res.man.nrm)

# Non parameteric
# wilcox.test(x, y, paired = FALSE, alternative ="two.sided")


```

Testing more than two groups at once.

Kruskal-Wallis test: non-parametric ANOVA (min. 3 samples), test tends to be significant if the p-value is < 0.1 or better 0.05

```{r}

#bindedGroups.K3.XY <- merge(bindedGroups.K3.X, waterY, by = "Events", all = T)

x<-c(600, 100, 1000, 2200, 2900, 2400, 700, 2000, 1700)
y<-c(200, 100, 200, 800, 1000, 700, 1800, 1900, 2400)
z<-c(0, 0, 3000, 0, 1000, 0, 3000, 1000, 0)

# Non-parameteric
kruskal.test(x, y, z)
```


## Plotting K-nodes vs. node height

```{r}
# Hellinger distance based clustering
# windows(8,8)
## par(mfrow=c(2,2))

whichDistPlot <- function(x, x.hw){
  plot(x.hw$height, nrow(x):2, type="S", main="Ward/Hellinger",
       ylab="k (number of clusters)", xlab="h (node height)", col="grey")
  text(x.hw$height, nrow(x):2, nrow(x):2, col="red", cex=0.6)
}

# whichDistPlot(waterY, resWater.hw)
whichDistPlot(waterX, water.hell.clust)
whichDistPlot(waterX, water.norm.clust)

```






